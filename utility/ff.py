import matplotlib.pyplot as plt
import torch
import torch.nn as nn
from tqdm import tqdm
from torch.optim import Adam
from torchvision.datasets import MNIST
from torchvision.datasets import CIFAR10
from torchvision.transforms import Compose, ToTensor, Normalize, Lambda
from torch.utils.data import DataLoader, TensorDataset

# one hot encodings
masks = torch.eye(10)


class Net(torch.nn.Module):
    """
    A simple neural network with some hidden layer
    """

    def __init__(self, dims, training, device='cpu'):
        """
        Initialize the network
        :param dims: dimensions of the network
        :param training: training genetic function used to train the network
        :param device: device used for computing
        """
        super().__init__()
        self.layers = []
        self.device = device
        for d in range(len(dims) - 1):
            self.layers += [Layer(dims[d], dims[d + 1], training, device=device)]

    def predict(self, x):
        """
        Predict the label of the input
        :param x: data to predict
        :return: predicted label
        """
        goodness_per_label = []
        for label in range(10):
            label = torch.repeat_interleave(torch.tensor(label), x.shape[0]).to(self.device)
            h = add_symba_mask(x, label)  # overlay_y_on_x_cifar(x, label)

            goodness = []
            for i in range(0, len(self.layers)):
                layer = self.layers[i]
                h = layer(h)
                if i > 0:
                    goodness += [h.pow(2).mean(1)]
            goodness_per_label += [sum(goodness).unsqueeze(1)]
        goodness_per_label = torch.cat(goodness_per_label, 1)
        return goodness_per_label.argmax(1)

    def train(self, x_pos, x_neg, epochs=1000):
        """
        Train the network
        :param x_pos: positive data
        :param x_neg: negative data
        :param pretrain_first: true if use classical forward-forward training for the first layer
        """
        h_pos, h_neg = x_pos, x_neg
        for i, layer in enumerate(self.layers):
            print('training layer', i, '...')
            h_pos, h_neg = layer.train(h_pos, h_neg, epochs)

    def ff_train(self, x_pos, x_neg, epochs=1000):
        h_pos, h_neg = x_pos, x_neg
        for i, layer in enumerate(self.layers):
            print('training layer', i, '...')
            h_pos, h_neg = layer.ff_train(h_pos, h_neg, epochs)


class Layer(nn.Linear):
    """
    A layer of the network
    """

    def __init__(self, in_features, out_features, training, bias=True, device=None, dtype=None):
        """
        Initialize the layer
        :param in_features: number of input features
        :param out_features: number of output features
        :param training: training genetic function used to train the layer
        :param bias: bias of the network
        :param device: device used for computing
        :param dtype: type of the data
        """
        super().__init__(in_features, out_features, bias, device, dtype)
        self.relu = torch.nn.LeakyReLU()
        self.opt = Adam(self.parameters(), lr=0.3)
        self.threshold = 2.0
        self.training = training
        self.in_features = in_features
        self.out_features = out_features

    def forward(self, x):
        """
        Forward pass of the current layer
        :param x: input data
        :return: processed data
        """
        x_direction = x / (x.norm(2, 1, keepdim=True) + 1e-4)
        return self.relu(
            torch.mm(x_direction, self.weight.T) + self.bias.unsqueeze(0))

    def train(self, x_pos, x_neg, epochs=1000):
        """
        Train the layer with full batch and genetic algorithm
        :param x_pos: positive data
        :param x_neg: negative data
        :return: processed data
        """
        weights, bias = self.training(x_pos, x_neg, self.out_features, ngen=epochs)
        self.weight = nn.Parameter(weights)
        self.bias = nn.Parameter(bias)
        return self.forward(x_pos).detach(), self.forward(x_neg).detach()

    def ff_train(self, x_pos, x_neg, epochs=1000):
        """
        Train the layer with full batch and legacy forward-forward
        :param x_pos: positive data
        :param x_neg: negative data
        :return: processed data
        """
        for _ in tqdm(range(epochs)):
            g_pos = self.forward(x_pos).pow(2).mean(1)
            g_neg = self.forward(x_neg).pow(2).mean(1)

            delta = g_pos - g_neg
            alpha = 4
            delta = -alpha * delta
            exp = torch.exp(delta)
            loss = torch.log(1 + exp).mean()

            self.opt.zero_grad()
            # this backward just compute the derivative and hence
            # is not considered backpropagation.
            loss.backward()
            self.opt.step()
        return self.forward(x_pos).detach(), self.forward(x_neg).detach()


def MNIST_loaders(train_batch_size=50000, test_batch_size=10000):
    transform = Compose([
        ToTensor(),
        Normalize((0.1307,), (0.3081,)),
        Lambda(lambda x: torch.flatten(x))])

    train_loader = DataLoader(
        MNIST('./data/', train=True,
              download=True,
              transform=transform),
        batch_size=train_batch_size, shuffle=True)

    test_loader = DataLoader(
        MNIST('./data/', train=False,
              download=True,
              transform=transform),
        batch_size=test_batch_size, shuffle=False)

    return train_loader, test_loader


def CIFAR10_loaders(train_batch_size=50000, test_batch_size=10000):
    transform = Compose([
        ToTensor(),
        Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),
        Lambda(lambda x: torch.flatten(x))])

    train_loader = DataLoader(
        CIFAR10('./data/', train=True, download=True, transform=transform),
        batch_size=train_batch_size, shuffle=True
    )

    test_loader = DataLoader(
        CIFAR10('./data/', train=False, download=True, transform=transform),
        batch_size=test_batch_size, shuffle=False
    )

    return train_loader, test_loader


def add_symba_mask(x, y):
    """
    Concatenate a one-hot encoding vector to the input, following the SYMBA paper
    :param x: original input
    :param y: label of the input
    :return: original input concatenated with the one-hot encoding vector
    """
    x_ = x.clone()

    # get the one-hot encoding vector
    m = masks.to(x.device)[y]

    # concatenate the one-hot encoding vector to the input
    return torch.cat((x_, m), dim=1)


def get_x_pos_xneg(train_loader, device):
    """ Get the positive and negative examples """

    # assuming the loader has a batch big as the full dataset
    x, y = next(iter(train_loader))

    # get a random labels for the negative examples
    rnd = torch.randperm(x.size(0))

    # add the one-hot encoding vector to the positive and negative examples
    x_pos = add_symba_mask(x, y)
    x_neg = add_symba_mask(x, y[rnd])

    return x_pos.to(device), x_neg.to(device)

#%%
